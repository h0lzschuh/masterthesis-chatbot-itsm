{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confluence Markdown Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFLUENCE_URL = 'https://your-confluence-url.com'\n",
    "USERNAME = 'your-username'\n",
    "API_TOKEN = 'your-api-token'\n",
    "SPACE_KEY = 'YOUR_SPACE_KEY'\n",
    "OUTPUT_DIR = \"export_directory\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from atlassian import Confluence\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "# Initialize Confluence API\n",
    "confluence = Confluence(\n",
    "    url=CONFLUENCE_URL,\n",
    "    username=USERNAME,\n",
    "    password=API_TOKEN\n",
    ")\n",
    "\n",
    "def sanitize_filename(title, max_length=50):\n",
    "    \"\"\"Sanitizes filenames and limits length.\"\"\"\n",
    "    sanitized = re.sub(r'[\\\\/:*?\"<>|]', '_', title)\n",
    "    return sanitized[:max_length] + \"...\" if len(sanitized) > max_length else sanitized\n",
    "\n",
    "def process_html_content(page_id, title, html_content):\n",
    "    \"\"\"Process HTML content, removing images and other non-textual elements.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove all images\n",
    "    for img in soup.find_all('img'):\n",
    "        img.decompose()\n",
    "    \n",
    "    # Remove all ac:image elements\n",
    "    for ac_image in soup.find_all('ac:image'):\n",
    "        ac_image.decompose()\n",
    "    \n",
    "    # Remove all draw.io diagrams\n",
    "    for drawio in soup.find_all('div', attrs={'data-macro-name': 'drawio'}):\n",
    "        drawio.decompose()\n",
    "    \n",
    "    # Remove any other attachment-related elements\n",
    "    for attachment in soup.find_all('ri:attachment'):\n",
    "        attachment.decompose()\n",
    "    \n",
    "    # Remove any file-related elements\n",
    "    for file_elem in soup.find_all('ac:structured-macro', {'ac:name': 'attachments'}):\n",
    "        file_elem.decompose()\n",
    "    \n",
    "    # Convert the modified HTML to markdown\n",
    "    modified_html = str(soup)\n",
    "    markdown_content = md(modified_html)\n",
    "    \n",
    "    return markdown_content\n",
    "\n",
    "def export_space_to_markdown(space_key, output_dir):\n",
    "    \"\"\"Exports all pages in a Confluence space to Markdown with only textual content.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Use pagination to get all pages\n",
    "    start = 0\n",
    "    limit = 100  # Smaller batch size for better error handling\n",
    "    all_pages = []\n",
    "    \n",
    "    while True:\n",
    "        batch = confluence.get_all_pages_from_space(space_key, start=start, limit=limit)\n",
    "        if not batch:\n",
    "            break\n",
    "            \n",
    "        all_pages.extend(batch)\n",
    "        start += limit\n",
    "        print(f\"Retrieved {len(all_pages)} pages so far...\")\n",
    "        \n",
    "        # If we got fewer pages than the limit, we've reached the end\n",
    "        if len(batch) < limit:\n",
    "            break\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"Total pages found: {len(all_pages)}\")\n",
    "    \n",
    "    for page in all_pages:\n",
    "        title = page['title']\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        markdown_file_path = os.path.join(output_dir, f\"{sanitized_title}.md\")\n",
    "\n",
    "        if os.path.exists(markdown_file_path):\n",
    "            print(f\"‚ö†Ô∏è  Skipped (already exported): {title}\")\n",
    "            continue\n",
    "\n",
    "        if len(markdown_file_path) > 255:\n",
    "            print(f\"‚ùå Error: File path too long -> {markdown_file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            page_content = confluence.get_page_by_id(page['id'], expand='body.storage')\n",
    "            html_content = page_content['body']['storage']['value']\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error retrieving page '{title}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Process HTML content to remove images and other non-textual elements\n",
    "            updated_markdown = process_html_content(page['id'], title, html_content)\n",
    "\n",
    "            with (markdown_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(updated_markdown)\n",
    "            print(f\"‚úÖ Exported: {title}\")\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing page '{title}': {e}\")\n",
    "        \n",
    "        # Add a small delay between page processing to avoid rate limiting\n",
    "        time.sleep(0.2)\n",
    "\n",
    "# Run the export\n",
    "if __name__ == \"__main__\":\n",
    "    export_space_to_markdown(SPACE_KEY, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFLUENCE_URL = 'https://your-confluence-url.com'\n",
    "USERNAME = 'your-username'\n",
    "API_TOKEN = 'your-api-token'\n",
    "\n",
    "#your-api-token\n",
    "SPACE_KEY = 'YOUR KEY'\n",
    "OUTPUT_DIR = \"export_directory\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from atlassian import Confluence\n",
    "from markdownify import markdownify as md\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "# Initialize Confluence API\n",
    "confluence = Confluence(\n",
    "    url=CONFLUENCE_URL,\n",
    "    username=USERNAME,\n",
    "    password=API_TOKEN\n",
    ")\n",
    "\n",
    "def sanitize_filename(title, max_length=50):\n",
    "    \"\"\"Sanitizes filenames and limits length.\"\"\"\n",
    "    sanitized = re.sub(r'[\\\\/:*?\"<>|]', '_', title)\n",
    "    return sanitized[:max_length] + \"...\" if len(sanitized) > max_length else sanitized\n",
    "\n",
    "def create_image_filename(page_title, counter, max_length=30):\n",
    "    \"\"\"Create an image filename based on page title with only letters, max 30 chars.\"\"\"\n",
    "    # Remove non-letter characters and convert to lowercase\n",
    "    letters_only = ''.join(c for c in page_title if c.isalpha())\n",
    "    \n",
    "    # If counter is provided, add it as prefix\n",
    "    if counter > 0:\n",
    "        prefix = f\"{counter}\"\n",
    "        # Ensure total length doesn't exceed max_length\n",
    "        available_length = max_length - len(prefix)\n",
    "        if available_length <= 0:\n",
    "            # If prefix is too long, just use the prefix\n",
    "            return prefix[:max_length]\n",
    "        # Use prefix + truncated title\n",
    "        return f\"{prefix}{letters_only[:available_length]}\"\n",
    "    else:\n",
    "        # Just use the title, truncated if needed\n",
    "        return letters_only[:max_length]\n",
    "\n",
    "def get_file_extension(filename):\n",
    "    \"\"\"Extract the file extension from a filename.\"\"\"\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    return ext.lower() if ext else \".png\"  # Default to .png if no extension\n",
    "\n",
    "def get_attachments(page_id):\n",
    "    \"\"\"Retrieves a list of all attachments for a given Confluence page.\"\"\"\n",
    "    attachments = []\n",
    "    start = 0\n",
    "    limit = 50\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{CONFLUENCE_URL}/rest/api/content/{page_id}/child/attachment?start={start}&limit={limit}\"\n",
    "        response = requests.get(url, auth=(USERNAME, API_TOKEN), headers={\"Accept\": \"application/json\"})\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Failed to fetch attachments for page {page_id}: {response.status_code}\")\n",
    "            break\n",
    "            \n",
    "        result = response.json()\n",
    "        batch = result.get('results', [])\n",
    "        attachments.extend(batch)\n",
    "        \n",
    "        if len(batch) < limit or 'next' not in result['_links']:\n",
    "            break\n",
    "            \n",
    "        start += limit\n",
    "    \n",
    "    return attachments\n",
    "\n",
    "def extract_diagram_id_from_filename(filename):\n",
    "    \"\"\"Extract the diagram ID from a filename (e.g., 'Untitled Diagram-1742929746623').\"\"\"\n",
    "    match = re.search(r'Diagram-(\\d+)', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def find_png_for_drawio(diagram_id, attachments):\n",
    "    \"\"\"Find the PNG file for a draw.io diagram based on its ID.\"\"\"\n",
    "    if not diagram_id:\n",
    "        return None\n",
    "    \n",
    "    for attachment in attachments:\n",
    "        title = attachment['title']\n",
    "        if title.lower().endswith('.png') and diagram_id in title:\n",
    "            return attachment\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_image_references_from_html(html_content):\n",
    "    \"\"\"Extract all image references from HTML content.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    image_refs = {}\n",
    "    \n",
    "    for img in soup.find_all('img'):\n",
    "        src = img.get('src', '')\n",
    "        if src:\n",
    "            filename = os.path.basename(unquote(urlparse(src).path))\n",
    "            image_refs[filename] = src\n",
    "    \n",
    "    for drawio in soup.find_all('div', attrs={'data-macro-name': 'drawio'}):\n",
    "        for attr, value in drawio.attrs.items():\n",
    "            if attr.startswith('data-') and value:\n",
    "                image_refs[value] = value\n",
    "        \n",
    "        img = drawio.find('img')\n",
    "        if img and img.get('src'):\n",
    "            src = img.get('src')\n",
    "            filename = os.path.basename(unquote(urlparse(src).path))\n",
    "            image_refs[filename] = src\n",
    "    \n",
    "    for ac_image in soup.find_all('ac:image'):\n",
    "        ri_value = ac_image.find('ri:attachment', {'ri:filename': True})\n",
    "        if ri_value:\n",
    "            filename = ri_value.get('ri:filename')\n",
    "            if filename:\n",
    "                image_refs[filename] = filename\n",
    "    \n",
    "    return image_refs\n",
    "\n",
    "def process_html_content(page_id, page_title, html_content, images_dir):\n",
    "    \"\"\"Process HTML content, download images with unique IDs, and create markdown with proper references.\"\"\"\n",
    "    # Get all attachments for the page\n",
    "    attachments = get_attachments(page_id)\n",
    "    attachment_map = {att['title']: att for att in attachments}\n",
    "    \n",
    "    # Extract image references from HTML\n",
    "    image_refs = extract_image_references_from_html(html_content)\n",
    "    \n",
    "    # Track downloaded files and their unique IDs\n",
    "    filename_to_unique_path = {}\n",
    "    id_to_filename_map = {}  # For logging/debugging\n",
    "    drawio_id_to_png_map = {}  # Map draw.io IDs to their PNG files\n",
    "    \n",
    "    # First pass: identify draw.io diagrams and their PNG representations\n",
    "    for filename, attachment in attachment_map.items():\n",
    "        # Check if this is a draw.io file\n",
    "        is_drawio = (filename.lower().endswith('.drawio') or \n",
    "                    'draw.io' in filename.lower() or \n",
    "                    filename.endswith('.tmp'))\n",
    "        \n",
    "        if is_drawio:\n",
    "            # Extract the diagram ID\n",
    "            diagram_id = extract_diagram_id_from_filename(filename)\n",
    "            if diagram_id:\n",
    "                # Find the corresponding PNG file\n",
    "                png_attachment = find_png_for_drawio(diagram_id, attachments)\n",
    "                if png_attachment:\n",
    "                    drawio_id_to_png_map[diagram_id] = png_attachment['title']\n",
    "                    print(f\"üîó Found PNG for diagram ID {diagram_id}: {png_attachment['title']}\")\n",
    "    \n",
    "    # Make sure images directory exists\n",
    "    if not os.path.exists(images_dir):\n",
    "        os.makedirs(images_dir)\n",
    "    \n",
    "    # Counter for images from this page\n",
    "    image_counter = 0\n",
    "    \n",
    "    # Download all images and PNG representations\n",
    "    for filename, attachment in attachment_map.items():\n",
    "        # Skip non-image files that aren't referenced\n",
    "        is_image = filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp'))\n",
    "        if not is_image and filename not in image_refs:\n",
    "            continue\n",
    "        \n",
    "        # Increment counter for each image from this page\n",
    "        image_counter += 1\n",
    "        \n",
    "        # Create filename based on page title with a counter\n",
    "        base_filename = create_image_filename(page_title, image_counter)\n",
    "        file_ext = get_file_extension(filename)\n",
    "        unique_filename = f\"{base_filename}{file_ext}\"\n",
    "        \n",
    "        download_url = attachment['_links']['download']\n",
    "        full_url = f\"{CONFLUENCE_URL}{download_url}\"\n",
    "        \n",
    "        local_path = os.path.join(images_dir, unique_filename)\n",
    "        \n",
    "        # Download the file\n",
    "        try:\n",
    "            response = requests.get(full_url, auth=(USERNAME, API_TOKEN), stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(local_path, 'wb') as file:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        file.write(chunk)\n",
    "                print(f\"üì• Downloaded: {filename} as {unique_filename}\")\n",
    "                \n",
    "                # Store the relative path for markdown\n",
    "                rel_path = os.path.join(\"images\", unique_filename)\n",
    "                filename_to_unique_path[filename] = rel_path\n",
    "                id_to_filename_map[base_filename] = filename  # For reference\n",
    "                \n",
    "                # If this is a PNG for a draw.io diagram, store the diagram ID -> unique path mapping\n",
    "                diagram_id = extract_diagram_id_from_filename(filename)\n",
    "                if diagram_id and filename.lower().endswith('.png'):\n",
    "                    drawio_id_to_png_map[diagram_id] = rel_path\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to download: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading {filename}: {e}\")\n",
    "        \n",
    "        time.sleep(1)  # Be nice to the server\n",
    "    \n",
    "    # Create a mapping file to track original filenames to unique IDs\n",
    "    mapping_file_path = os.path.join(images_dir, \"_image_mapping.txt\")\n",
    "    try:\n",
    "        with open(mapping_file_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"\\n--- Page: {page_title} ---\\n\")\n",
    "            f.write(\"Original Filename -> New Filename\\n\")\n",
    "            f.write(\"===========================\\n\")\n",
    "            for original_name, local_path in filename_to_unique_path.items():\n",
    "                new_filename = os.path.basename(local_path)  # Extract filename from path\n",
    "                f.write(f\"{original_name} -> {new_filename}\\n\")\n",
    "            \n",
    "            # Also document draw.io ID to PNG mappings\n",
    "            if drawio_id_to_png_map:\n",
    "                f.write(\"\\nDraw.io ID -> PNG Path\\n\")\n",
    "                f.write(\"===========================\\n\")\n",
    "                for diagram_id, png_path in drawio_id_to_png_map.items():\n",
    "                    f.write(f\"{diagram_id} -> {png_path}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error writing mapping file: {e}\")\n",
    "    \n",
    "    # Now process HTML to replace image references before converting to markdown\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Replace image sources in the HTML\n",
    "    for img in soup.find_all('img'):\n",
    "        src = img.get('src', '')\n",
    "        if src:\n",
    "            filename = os.path.basename(unquote(urlparse(src).path))\n",
    "            if filename in filename_to_unique_path:\n",
    "                img['src'] = filename_to_unique_path[filename]\n",
    "    \n",
    "    # Replace draw.io diagrams with their PNG representations\n",
    "    for drawio in soup.find_all('div', attrs={'data-macro-name': 'drawio'}):\n",
    "        # Try to find the diagram ID from attributes\n",
    "        diagram_id = None\n",
    "        for attr, value in drawio.attrs.items():\n",
    "            if attr.startswith('data-') and 'diagram' in attr.lower():\n",
    "                # Try to extract a diagram ID from the value\n",
    "                id_match = re.search(r'(\\d{10,})', value)\n",
    "                if id_match:\n",
    "                    diagram_id = id_match.group(1)\n",
    "                    break\n",
    "        \n",
    "        # If we didn't find an ID in the attributes, try the content\n",
    "        if not diagram_id:\n",
    "            # Check if there's text content that might contain the ID\n",
    "            text = drawio.get_text()\n",
    "            id_match = re.search(r'(\\d{10,})', text)\n",
    "            if id_match:\n",
    "                diagram_id = id_match.group(1)\n",
    "        \n",
    "        # If we found a diagram ID and have a PNG for it, replace the draw.io div\n",
    "        if diagram_id and diagram_id in drawio_id_to_png_map:\n",
    "            png_path = drawio_id_to_png_map[diagram_id]\n",
    "            new_img = soup.new_tag('img')\n",
    "            new_img['src'] = png_path\n",
    "            new_img['alt'] = f\"Draw.io diagram {diagram_id}\"\n",
    "            drawio.replace_with(new_img)\n",
    "            print(f\"üîÑ Replaced draw.io diagram {diagram_id} with PNG image\")\n",
    "        else:\n",
    "            # If we can't find a match, add a comment so we can find it later\n",
    "            drawio['data-export-note'] = \"Unable to find PNG representation\"\n",
    "    \n",
    "    # Replace ac:image elements\n",
    "    for ac_image in soup.find_all('ac:image'):\n",
    "        ri_value = ac_image.find('ri:attachment', {'ri:filename': True})\n",
    "        if ri_value:\n",
    "            filename = ri_value.get('ri:filename')\n",
    "            if filename and filename in filename_to_unique_path:\n",
    "                new_img = soup.new_tag('img')\n",
    "                new_img['src'] = filename_to_unique_path[filename]\n",
    "                ac_image.replace_with(new_img)\n",
    "    \n",
    "    # Convert the modified HTML to markdown\n",
    "    modified_html = str(soup)\n",
    "    markdown_content = md(modified_html)\n",
    "    \n",
    "    # Post-process the markdown to fix any remaining issues\n",
    "    \n",
    "    # 1. Fix standard image references\n",
    "    for original_name, local_path in filename_to_unique_path.items():\n",
    "        markdown_content = markdown_content.replace(f\"![]({original_name})\", f\"![]({local_path})\")\n",
    "        markdown_content = markdown_content.replace(f\"![{original_name}]({original_name})\", f\"![]({local_path})\")\n",
    "        \n",
    "        # Also check for full URLs\n",
    "        full_url_pattern = f\"{CONFLUENCE_URL}/download/attachments/{page_id}/{original_name}\"\n",
    "        markdown_content = markdown_content.replace(f\"![]({full_url_pattern})\", f\"![]({local_path})\")\n",
    "        markdown_content = markdown_content.replace(f\"![{original_name}]({full_url_pattern})\", f\"![]({local_path})\")\n",
    "    \n",
    "    # 2. Fix draw.io specific patterns\n",
    "    # Look for patterns like \"trueUntitled Diagram-1742929746623falseautoptrue61811\"\n",
    "    drawio_pattern = r'true([^f]+)false[^t]*true\\d+'\n",
    "    for match in re.finditer(drawio_pattern, markdown_content):\n",
    "        full_match = match.group(0)\n",
    "        diagram_text = match.group(1).strip()\n",
    "        \n",
    "        # Try to extract the diagram ID\n",
    "        id_match = re.search(r'(\\d{10,})', diagram_text)\n",
    "        if id_match:\n",
    "            diagram_id = id_match.group(1)\n",
    "            if diagram_id in drawio_id_to_png_map:\n",
    "                png_path = drawio_id_to_png_map[diagram_id]\n",
    "                markdown_content = markdown_content.replace(full_match, f\"![]({png_path})\")\n",
    "                print(f\"üîç Replaced draw.io pattern for diagram {diagram_id}\")\n",
    "    \n",
    "    # 3. Remove any remaining draw.io artifacts\n",
    "    markdown_content = re.sub(r'true[^f]*false[^t]*true\\d+', '', markdown_content)\n",
    "    \n",
    "    return markdown_content\n",
    "\n",
    "def export_space_to_markdown(space_key, output_dir):\n",
    "    \"\"\"Exports all pages in a Confluence space to Markdown with uniquely identified images.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    images_dir = os.path.join(output_dir, \"images\")\n",
    "    if not os.path.exists(images_dir):\n",
    "        os.makedirs(images_dir)\n",
    "\n",
    "    pages = confluence.get_all_pages_from_space(space_key, start=0, limit=1000)\n",
    "    \n",
    "    for page in pages:\n",
    "        title = page['title']\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        markdown_file_path = os.path.join(output_dir, f\"{sanitized_title}.md\")\n",
    "\n",
    "        if os.path.exists(markdown_file_path):\n",
    "            print(f\"‚ö†Ô∏è  Skipped (already exported): {title}\")\n",
    "            continue\n",
    "\n",
    "        if len(markdown_file_path) > 255:\n",
    "            print(f\"‚ùå Error: File path too long -> {markdown_file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            page_content = confluence.get_page_by_id(page['id'], expand='body.storage')\n",
    "            html_content = page_content['body']['storage']['value']\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error retrieving page '{title}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            updated_markdown = process_html_content(page['id'], title, html_content, images_dir)\n",
    "\n",
    "            with open(markdown_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(updated_markdown)\n",
    "            print(f\"‚úÖ Exported: {title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing page '{title}': {e}\")\n",
    "\n",
    "# Run the export\n",
    "if __name__ == \"__main__\":\n",
    "    export_space_to_markdown(SPACE_KEY, OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
